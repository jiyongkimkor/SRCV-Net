{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90c02207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-18 08:59:04.043321: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2')\n",
      "\n",
      "\n",
      " Running on multiple GPUs  ['/device:GPU:0', '/device:GPU:1', '/device:GPU:2']\n"
     ]
    },
    {
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.config.experimental import list_logical_devices\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 3, 4\"\n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "tf.config.experimental.set_memory_growth(gpus[1], True)\n",
    "tf.config.experimental.set_memory_growth(gpus[2], True)\n",
    "\n",
    " \n",
    "gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy([gpus[0].name, gpus[1].name, gpus[2].name])\n",
    "print('\\n\\n Running on multiple GPUs ', [gpus[0].name, gpus[1].name, gpus[2].name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a1812da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# read image\n",
    "\n",
    "disp_train_path = '/root/check/matching/datasets/US3D/experimental_data/disp/Track2-Truth/'\n",
    "\n",
    "train_dataset_path = '/root/check/matching/datasets/US3D/experimental_data/left/Track2-RGB-1/'\n",
    "all_left_paths, all_right_paths, all_disp_paths = [], [], []\n",
    "\n",
    "for filename in os.listdir(train_dataset_path):\n",
    "    if 'LEFT_RGB' in filename:\n",
    "        left_path = os.path.join(train_dataset_path, filename)\n",
    "        right_path = os.path.join(train_dataset_path, filename.replace('LEFT_RGB', 'RIGHT_RGB'))\n",
    "        disp_path = os.path.join(disp_train_path, filename.replace('LEFT_RGB', 'LEFT_DSP'))\n",
    "        \n",
    "        all_left_paths.append(left_path)\n",
    "        all_right_paths.append(right_path)\n",
    "        all_disp_paths.append(disp_path)\n",
    "    \n",
    "val_dataset_path = '/root/check/matching/datasets/US3D/experimental_data/left/Track2-RGB-2/'\n",
    "val_all_left_paths, val_all_right_paths, val_all_disp_paths = [], [], []\n",
    "\n",
    "for filename in os.listdir(val_dataset_path):\n",
    "    if 'LEFT_RGB' in filename:\n",
    "        val_left_path = os.path.join(val_dataset_path, filename)\n",
    "        val_right_path = os.path.join(val_dataset_path, filename.replace('LEFT_RGB', 'RIGHT_RGB'))\n",
    "        val_disp_path = os.path.join(disp_train_path, filename.replace('LEFT_RGB', 'LEFT_DSP'))\n",
    "        \n",
    "        val_all_left_paths.append(val_left_path)\n",
    "        val_all_right_paths.append(val_right_path)\n",
    "        val_all_disp_paths.append(val_disp_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3220072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from SRCVNet.srcvnet import *\n",
    "from SRCVNet.data_reader import *\n",
    "from SRCVNet.evaluation import *\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "min_disp = -96\n",
    "max_disp = 96\n",
    "\n",
    "# Train the model\n",
    "batch_size = 6\n",
    "epochs = 100\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "def learning_rate_decay(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    elif epoch % 10 == 0:\n",
    "        return lr * 0.5  # Explicitly cast to float\n",
    "    else: \n",
    "        return lr\n",
    "\n",
    "# Create the LearningRateScheduler callback\n",
    "lr_decay_callback = LearningRateScheduler(learning_rate_decay)\n",
    "# Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48010a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {\n",
    "    'estimation': sl1_8x,\n",
    "    'estimation_1': sl1_4x,\n",
    "    'refinement': sl1_1x,\n",
    "    #'refinement_1': smooth_l1_loss_with_coefficient,\n",
    "}\n",
    "\n",
    "loss_weights = {\n",
    "    'estimation': 0.6,\n",
    "    'estimation_1': 1.0,\n",
    "    'refinement': 0.7,\n",
    "    #'refinement_1': 1.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddf97bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epe = epe\n",
    "d1 = d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6d6ddd3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1024, 1024,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1024, 1024,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "feature_extraction (FeatureExtr [(None, 256, 256, 16 202128      input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cost_concatenation_1 (CostConca (None, 24, 128, 128, 0           feature_extraction[0][1]         \n",
      "                                                                 feature_extraction[1][1]         \n",
      "                                                                 feature_extraction[0][1]         \n",
      "                                                                 feature_extraction[0][1]         \n",
      "__________________________________________________________________________________________________\n",
      "cost_concatenation (CostConcate (None, 48, 256, 256, 0           feature_extraction[0][0]         \n",
      "                                                                 feature_extraction[1][0]         \n",
      "                                                                 feature_extraction[0][0]         \n",
      "                                                                 feature_extraction[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "cost_refinement_1 (cost_refinem (None, 24, 128, 128, 57728       cost_concatenation_1[0][0]       \n",
      "                                                                 cost_concatenation_1[1][0]       \n",
      "__________________________________________________________________________________________________\n",
      "cost_refinement (cost_refinemen (None, 48, 256, 256, 57728       cost_concatenation[0][0]         \n",
      "                                                                 cost_concatenation[1][0]         \n",
      "__________________________________________________________________________________________________\n",
      "hourglass_1 (Hourglass)         (None, 24, 128, 128, 216640      cost_refinement_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "hourglass (Hourglass)           (None, 48, 256, 256, 216640      cost_refinement[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "efficient_attention_1 (Efficien (None, 24, 128, 128, 2688        hourglass_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "efficient_attention (EfficientA (None, 48, 256, 256, 9984        hourglass[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "feature_fusion (FeatureFusion)  (None, 48, 256, 256, 544         efficient_attention_1[0][0]      \n",
      "                                                                 efficient_attention[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "hourglass_2 (Hourglass)         (None, 48, 256, 256, 209728      feature_fusion[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "efficient_attention_2 (Efficien (None, 48, 256, 256, 9984        hourglass_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "estimation_1 (Estimation)       (None, 256, 256, 1)  7361        efficient_attention_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 1024, 1024,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 1024, 1024,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "estimation (Estimation)         (None, 128, 128, 1)  7361        hourglass_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "refinement (Refinement)         (None, 1024, 1024, 1 40673       estimation_1[0][0]               \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,039,187\n",
      "Trainable params: 1,034,899\n",
      "Non-trainable params: 4,288\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "# Compile the model with the custom losses and metrics\n",
    "with strategy.scope():\n",
    "    net = SRCVNet(1024, 1024, 3, min_disp=min_disp, max_disp=max_disp)\n",
    "    net.build_model()\n",
    "    net.model.compile(optimizer=Adam(learning_rate = learning_rate),\n",
    "                        loss=losses, \n",
    "                        loss_weights = loss_weights,\n",
    "                        metrics=[epe, d1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5494ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-18 08:59:20.982422: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.\n",
      "2024-09-18 08:59:20.982471: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.\n",
      "2024-09-18 08:59:20.982523: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1611] Profiler found 3 GPUs\n",
      "2024-09-18 08:59:21.002072: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcupti.so.11.3\n",
      "2024-09-18 08:59:21.438433: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.\n",
      "2024-09-18 08:59:21.438982: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1744] CUPTI activity buffer flushed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
      "2024-09-18 08:59:22.694997: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2024-09-18 08:59:22.717532: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2644800000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "train_generator = BatchLoader(all_left_paths, all_right_paths, all_disp_paths, batch_size=batch_size, reshuffle=True)\n",
    "validation_generator = BatchLoader(val_all_left_paths, val_all_right_paths, val_all_disp_paths, batch_size=batch_size, reshuffle=False)\n",
    "\n",
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "# Define a ModelCheckpoint callback\n",
    "\n",
    "steps_per_epoch = len(train_generator) // batch_size\n",
    "\n",
    "\n",
    "directory = \"/root/check/matching/checkpoint/US3D/SRCVNet_woEASRCV/\"\n",
    "\n",
    "tboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=directory ) \n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    os.path.join(directory, \"SRCVNet_checkpoint.h5py\"),\n",
    "    save_best_only=True, monitor='val_refinement_epe',# Save only the best model\n",
    ")\n",
    " \n",
    "loss_history = []\n",
    "accuracy_history = []\n",
    "\n",
    "# Train the model with the callback\n",
    "history = net.model.fit(train_generator, \n",
    "                        epochs=epochs, \n",
    "                        #steps_per_epoch = len(train_generator),\n",
    "                        validation_data=validation_generator,\n",
    "                        #validation_steps = len(validation_generator),\n",
    "                        callbacks=[lr_decay_callback, checkpoint_callback, tboard_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
